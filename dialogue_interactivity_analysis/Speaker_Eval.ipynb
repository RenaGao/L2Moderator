{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abe138c-3efb-4a6b-a1c2-5f2e8dc0e853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to evaluate each speaker's dialogue quality based on the previous criteria\n",
    "def evaluate_speaker_quality(dataframe):\n",
    "    # Get unique speakers\n",
    "    speakers = dataframe['speaker'].unique()\n",
    "    speaker_scores = []\n",
    "\n",
    "    # Evaluate each speaker by segmenting their dialogue and applying interactivity scores\n",
    "    for speaker in speakers:\n",
    "        # Filter data for each speaker\n",
    "        speaker_data = dataframe[dataframe['speaker'] == speaker]\n",
    "        \n",
    "        # Aggregate scores for each interactivity aspect\n",
    "        topic_management_scores = []\n",
    "        tone_appropriateness_scores = []\n",
    "        opening_scores = []\n",
    "        closing_scores = []\n",
    "\n",
    "        # Apply scoring to each segment of the speaker\n",
    "        for seg_id in speaker_data['segment'].unique():\n",
    "            segment_texts = speaker_data[speaker_data['segment'] == seg_id]['text'].tolist()\n",
    "            segment_roles = speaker_data[speaker_data['segment'] == seg_id]['role'].tolist()\n",
    "            # Score each segment with moderator and multi-party criteria\n",
    "            scores = score_interactivity_aspects_multiparty(segment_texts, segment_roles)\n",
    "            topic_management_scores.append(scores[0])\n",
    "            tone_appropriateness_scores.append(scores[1])\n",
    "            opening_scores.append(scores[2])\n",
    "            closing_scores.append(scores[3])\n",
    "        \n",
    "        # Calculate average scores for each interactivity aspect for the speaker\n",
    "        avg_topic_management = sum(topic_management_scores) / len(topic_management_scores) if topic_management_scores else 0\n",
    "        avg_tone_appropriateness = sum(tone_appropriateness_scores) / len(tone_appropriateness_scores) if tone_appropriateness_scores else 0\n",
    "        avg_opening = sum(opening_scores) / len(opening_scores) if opening_scores else 0\n",
    "        avg_closing = sum(closing_scores) / len(closing_scores) if closing_scores else 0\n",
    "        \n",
    "        # Append results\n",
    "        speaker_scores.append((speaker, avg_topic_management, avg_tone_appropriateness, avg_opening, avg_closing))\n",
    "    \n",
    "    # Create a DataFrame with the results\n",
    "    speaker_scores_df = pd.DataFrame(speaker_scores, columns=['Speaker', 'Topic Management', 'Tone Appropriateness', \n",
    "                                                              'Conversation Opening', 'Conversation Closing'])\n",
    "    return speaker_scores_df\n",
    "\n",
    "# Evaluate speaker quality for the first dialogue\n",
    "first_dialogue_speaker_scores = evaluate_speaker_quality(data)\n",
    "\n",
    "# Evaluate speaker quality for the second dialogue\n",
    "second_dialogue_speaker_scores = evaluate_speaker_quality(data_new)\n",
    "\n",
    "# Combine the results into one CSV file for easy download\n",
    "combined_speaker_scores = pd.concat([first_dialogue_speaker_scores.assign(Dialogue=\"First\"),\n",
    "                                     second_dialogue_speaker_scores.assign(Dialogue=\"Second\")])\n",
    "\n",
    "# Save to CSV\n",
    "file_path = '/mnt/data/Speaker_Dialogue_Quality_Scores.csv'\n",
    "combined_speaker_scores.to_csv(file_path, index=False)\n",
    "\n",
    "file_path\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

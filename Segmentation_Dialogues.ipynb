{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57cc87b-19f1-43cb-94a1-1d342411f374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset previous speaker and role variables for segmentation\n",
    "previous_speaker, previous_role = None, None\n",
    "segment_id = 0\n",
    "segments = []\n",
    "\n",
    "# Segment the new dialogue based on topic transitions using the same function\n",
    "for index, row in data_new.iterrows():\n",
    "    if detect_topic_transition(row, previous_speaker, previous_role):\n",
    "        segment_id += 1  # New segment\n",
    "    segments.append(segment_id)\n",
    "    \n",
    "    # Update previous speaker and role for the next iteration\n",
    "    previous_speaker, previous_role = row['speaker'], row['role']\n",
    "\n",
    "# Append the segment information to the new DataFrame\n",
    "data_new['segment'] = segments\n",
    "\n",
    "# Evaluate each segment in the new data using the same evaluation function\n",
    "segment_scores_new = []\n",
    "for segment_id in data_new['segment'].unique():\n",
    "    segment_texts = data_new[data_new['segment'] == segment_id]['text'].tolist()\n",
    "    score = evaluate_segment(segment_texts)\n",
    "    segment_scores_new.append((segment_id, score))\n",
    "\n",
    "# Create a DataFrame to display results for the new dialogue file\n",
    "evaluation_df_new = pd.DataFrame(segment_scores_new, columns=['Segment ID', 'Score'])\n",
    "\n",
    "# Display the evaluation results for the new dialogue\n",
    "tools.display_dataframe_to_user(name=\"New Segment Evaluation Results\", dataframe=evaluation_df_new)\n",
    "\n",
    "# Show the evaluation DataFrame for confirmation\n",
    "evaluation_df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb932bae-a032-45cf-9070-e64d584f2f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to evaluate each segment based on the specified criteria\n",
    "def evaluate_segment(segment_texts):\n",
    "    # Basic heuristics to determine scores\n",
    "    if all(\"?\" not in text for text in segment_texts):  # If no questions, likely a smoother, more declarative flow\n",
    "        if all(len(text) > 5 for text in segment_texts):  # Smooth conversation with longer utterances\n",
    "            return 5  # Score 5 for very smooth and fluent communication\n",
    "        else:\n",
    "            return 4  # Score 4 if slightly shorter, but still clear in communication\n",
    "    \n",
    "    # Check for hesitation or awkwardness in shorter exchanges or unclear questions/responses\n",
    "    if any(\"...\" in text or len(text) < 4 for text in segment_texts):  # Indication of hesitation or brevity\n",
    "        if any(\"?\" in text and len(text) < 5 for text in segment_texts):\n",
    "            return 3  # Score 3 for minor awkwardness or hesitation in communication\n",
    "    \n",
    "    # Awkward and less coherent communication based on length and response relevance\n",
    "    if any(\"?\" in text and \"...\" in text for text in segment_texts):\n",
    "        return 2  # Score 2 if multiple hesitations or awkward questions and responses\n",
    "    \n",
    "    # Score of 1 for mostly failed communication attempts\n",
    "    return 1\n",
    "\n",
    "# Apply segmentation and evaluation\n",
    "segment_scores = []\n",
    "for segment_id in data['segment'].unique():\n",
    "    segment_texts = data[data['segment'] == segment_id]['text'].tolist()\n",
    "    score = evaluate_segment(segment_texts)\n",
    "    segment_scores.append((segment_id, score))\n",
    "\n",
    "# Create a DataFrame to display results\n",
    "evaluation_df = pd.DataFrame(segment_scores, columns=['Segment ID', 'Score'])\n",
    "\n",
    "# Display the evaluation results to the user\n",
    "tools.display_dataframe_to_user(name=\"Segment Evaluation Results\", dataframe=evaluation_df)\n",
    "\n",
    "# Show the evaluation DataFrame for confirmation\n",
    "evaluation_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e763374-6943-4f83-a6c7-f1e73688b70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revised evaluation function to include new criteria for scoring\n",
    "def evaluate_segment_with_interactivity(segment_texts):\n",
    "    # Check for presence of certain interactive qualities for adjusted scoring\n",
    "    score = 1  # Start with lowest score and upgrade based on checks\n",
    "    \n",
    "    # Check for smooth openings and closings\n",
    "    if any(text.lower().startswith((\"hi\", \"hello\", \"good\", \"so\")) for text in segment_texts):\n",
    "        score = max(score, 3)  # Presence of conversational opening raises score\n",
    "\n",
    "    if any(text.lower().startswith((\"thanks\", \"thank you\", \"bye\", \"okay, that's it\")) for text in segment_texts):\n",
    "        score = max(score, 3)  # Presence of a conversational closing phrase raises score\n",
    "    \n",
    "    # Check for topic management cues like transitions\n",
    "    if any(text.lower() in [\"moving on\", \"next topic\", \"let's talk about\"] for text in segment_texts):\n",
    "        score = max(score, 4)  # Indicate smooth topic management\n",
    "    \n",
    "    # Check tone appropriateness and fluency\n",
    "    if all(len(text) > 5 for text in segment_texts):  # Smooth, continuous exchanges\n",
    "        score = max(score, 5)  # Highest fluency and appropriateness in tone with longer utterances\n",
    "    \n",
    "    # Check for awkward responses or signs of hesitation\n",
    "    if any(\"...\" in text or len(text) < 4 for text in segment_texts):  # Hesitation or brevity indicates awkwardness\n",
    "        score = min(score, 3)  # Reduce score if awkwardness is observed\n",
    "    \n",
    "    return score\n",
    "\n",
    "# Re-evaluate each segment in the first dialogue with updated criteria\n",
    "segment_scores_first_dialogue = []\n",
    "for segment_id in data['segment'].unique():\n",
    "    segment_texts = data[data['segment'] == segment_id]['text'].tolist()\n",
    "    score = evaluate_segment_with_interactivity(segment_texts)\n",
    "    segment_scores_first_dialogue.append((segment_id, score))\n",
    "\n",
    "# Create DataFrame for the re-evaluated scores of the first dialogue\n",
    "evaluation_df_first_dialogue = pd.DataFrame(segment_scores_first_dialogue, columns=['Segment ID', 'Score'])\n",
    "\n",
    "# Re-evaluate each segment in the second dialogue with updated criteria\n",
    "segment_scores_second_dialogue = []\n",
    "for segment_id in data_new['segment'].unique():\n",
    "    segment_texts = data_new[data_new['segment'] == segment_id]['text'].tolist()\n",
    "    score = evaluate_segment_with_interactivity(segment_texts)\n",
    "    segment_scores_second_dialogue.append((segment_id, score))\n",
    "\n",
    "# Create DataFrame for the re-evaluated scores of the second dialogue\n",
    "evaluation_df_second_dialogue = pd.DataFrame(segment_scores_second_dialogue, columns=['Segment ID', 'Score'])\n",
    "\n",
    "# Display the re-evaluation results for both dialogues\n",
    "import ace_tools as tools\n",
    "tools.display_dataframe_to_user(name=\"Re-evaluated Segment Scores - First Dialogue\", dataframe=evaluation_df_first_dialogue)\n",
    "tools.display_dataframe_to_user(name=\"Re-evaluated Segment Scores - Second Dialogue\", dataframe=evaluation_df_second_dialogue)\n",
    "\n",
    "evaluation_df_first_dialogue.head(), evaluation_df_second_dialogue.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6928b4f3-3c5c-4135-8851-5e82f75a232d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refine segmentation based on explicit topic transitions and concluding phrases\n",
    "def detect_topic_transition_v2(row, previous_text):\n",
    "    # Define explicit cues for topic transitions and conclusions\n",
    "    transition_cues = [\"moving on\", \"next topic\", \"now\", \"let's discuss\", \"let's move on\", \"to another point\"]\n",
    "    conclusion_cues = [\"so that's it\", \"okay then\", \"let's wrap up\", \"thanks\", \"that concludes\"]\n",
    "    \n",
    "    # Detect topic transition if the current text contains transition cues\n",
    "    if any(cue in row['text'].lower() for cue in transition_cues):\n",
    "        return True\n",
    "    # Detect end of a topic if the previous text contained conclusion cues\n",
    "    if previous_text and any(cue in previous_text.lower() for cue in conclusion_cues):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# Re-segment the first dialogue\n",
    "previous_text = None\n",
    "segment_id = 0\n",
    "segments_first = []\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    if detect_topic_transition_v2(row, previous_text):\n",
    "        segment_id += 1  # New segment\n",
    "    segments_first.append(segment_id)\n",
    "    previous_text = row['text']  # Update previous text\n",
    "\n",
    "data['segment'] = segments_first  # Update segment assignments in the DataFrame\n",
    "\n",
    "# Re-segment the second dialogue\n",
    "previous_text = None\n",
    "segment_id = 0\n",
    "segments_second = []\n",
    "\n",
    "for index, row in data_new.iterrows():\n",
    "    if detect_topic_transition_v2(row, previous_text):\n",
    "        segment_id += 1  # New segment\n",
    "    segments_second.append(segment_id)\n",
    "    previous_text = row['text']\n",
    "\n",
    "data_new['segment'] = segments_second  # Update segment assignments in the DataFrame\n",
    "\n",
    "# Re-evaluate each segment in the first dialogue with updated segmentation and criteria\n",
    "segment_scores_first_dialogue_v2 = []\n",
    "for segment_id in data['segment'].unique():\n",
    "    segment_texts = data[data['segment'] == segment_id]['text'].tolist()\n",
    "    score = evaluate_segment_with_interactivity(segment_texts)\n",
    "    segment_scores_first_dialogue_v2.append((segment_id, score))\n",
    "\n",
    "# DataFrame for re-evaluated scores of the first dialogue\n",
    "evaluation_df_first_dialogue_v2 = pd.DataFrame(segment_scores_first_dialogue_v2, columns=['Segment ID', 'Score'])\n",
    "\n",
    "# Re-evaluate each segment in the second dialogue with updated segmentation and criteria\n",
    "segment_scores_second_dialogue_v2 = []\n",
    "for segment_id in data_new['segment'].unique():\n",
    "    segment_texts = data_new[data_new['segment'] == segment_id]['text'].tolist()\n",
    "    score = evaluate_segment_with_interactivity(segment_texts)\n",
    "    segment_scores_second_dialogue_v2.append((segment_id, score))\n",
    "\n",
    "# DataFrame for re-evaluated scores of the second dialogue\n",
    "evaluation_df_second_dialogue_v2 = pd.DataFrame(segment_scores_second_dialogue_v2, columns=['Segment ID', 'Score'])\n",
    "\n",
    "# Display the re-evaluation results for both dialogues with new segmentation\n",
    "tools.display_dataframe_to_user(name=\"Re-evaluated Segment Scores - First Dialogue (Refined Segmentation)\", dataframe=evaluation_df_first_dialogue_v2)\n",
    "tools.display_dataframe_to_user(name=\"Re-evaluated Segment Scores - Second Dialogue (Refined Segmentation)\", dataframe=evaluation_df_second_dialogue_v2)\n",
    "\n",
    "evaluation_df_first_dialogue_v2.head(), evaluation_df_second_dialogue_v2.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
